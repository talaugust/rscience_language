{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM building functions\n",
    "This is a notebook just for function to build the language models. Since we used SRILM for building the models\n",
    "this has to be a notebook rather than a module I could import (we just jupyter magic in it to access SRILM).\n",
    "\n",
    "This notebook is run in pretty much every other notebook, since it has all the core functions in it. \n",
    "The functions have grown a little out of control, with lots of parameters. You can go into the other \n",
    "notebooks that import this to see example usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/taugust/ARK/community_guidelines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import seaborn as sns\n",
    "import uuid\n",
    "from scipy import stats\n",
    "import scikit_posthocs as sp\n",
    "from statsmodels import stats as sm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.util import bigrams, ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten, padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "os.chdir('/homes/gws/taugust/Projects/ARK/community_guidelines/')\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for LM building \n",
    "Take fron lib/SLM_building.py, added here so all the functions for SLM building could be in the same place\n",
    "the lib/ file is in the process of being phased out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = list(range(1,13)) \n",
    "\n",
    "# subs = ['AskHistorians', 'EverythingScience', 'Futurology', 'science', 'TrueReddit', 'dataisbeautiful', 'askscience']\n",
    "\n",
    "def print_row(row):\n",
    "    print(colored('----', 'red'), row)\n",
    "    \n",
    "def print_df_rows(df, sample=100, col='body'):\n",
    "    df.sample(sample)[col].apply(print_row)\n",
    "\n",
    "#### Gets entropy of passed text, uses data/tmp to store unique identifier\n",
    "def get_entropy(text, slms):\n",
    "    name = str(uuid.uuid4())\n",
    "    corpus_file = export_text(text, name=name, corpus_path='data/tmp/')\n",
    "    return calc_month_entropy_SRILM(slms, corpus_file)\n",
    "\n",
    "\n",
    "\n",
    "# if no month, assume you want the entire df and set threshold for total counts\n",
    "def get_active_users(author_df, month, author_col, threshold=5, num_authors=200, kind=None):\n",
    "    if kind:\n",
    "        author_df = author_df[author_df['kind'] == kind]  \n",
    "    if month is not None:\n",
    "        author_df = author_df[author_df[str(month)] >= threshold].drop_duplicates()\n",
    "    else:\n",
    "        print('No month...taking all author counts')\n",
    "        author_df = author_df[author_df[[str(m) for m in MONTHS]].sum(axis=1) >= threshold].drop_duplicates() \n",
    "    if num_authors: \n",
    "        return author_df.sample(num_authors, replace=True)[author_col]\n",
    "    return author_df[author_col]\n",
    "\n",
    "\n",
    "# if no month, assume you want the entire df and set threshold for total counts\n",
    "def get_first_active_users(author_df, month, author_col, threshold=5, num_authors=200, kind=None):\n",
    "    if kind:\n",
    "        author_df = author_df[author_df['kind'] == kind]  \n",
    "    if month is not None:\n",
    "        #####\n",
    "        # ignore all authors who never wrote before this month\n",
    "        ####\n",
    "        prev_months = range(1, month)\n",
    "        author_df =  author_df[author_df[[str(prev_m) for prev_m in prev_months]].sum(axis=1) == 0]\n",
    "        author_df = author_df[author_df[str(month)] >= threshold].drop_duplicates()\n",
    "    else:\n",
    "        print('No month...taking all author counts')\n",
    "        author_df = author_df[author_df[[str(m) for m in MONTHS]].sum(axis=1) >= threshold].drop_duplicates() \n",
    "    if num_authors: \n",
    "        return author_df.sample(num_authors, replace=True)[author_col]\n",
    "    return author_df[author_col]\n",
    "    \n",
    "def get_outside_users(author_df, month, author_col, threshold=1, num_authors=None, kind=None):\n",
    "    if kind:\n",
    "        author_df = author_df[author_df['kind'] == kind]\n",
    "    outside_users = author_df[author_df[[str(m) for m in MONTHS]].sum(axis=1) <= threshold]\n",
    "    if month is not None:\n",
    "        outside_users = outside_users[outside_users[str(month)] == threshold]\n",
    "    else:\n",
    "        print('No month...taking all author counts')    \n",
    "    if num_authors:\n",
    "        return outside_users[author_col].sample(num_authors, replace=True)\n",
    "    return outside_users[author_col]\n",
    "    \n",
    "##################################################################   \n",
    "# For each of these 200 active users\n",
    "# select 5 random 10-word spans from 5 unique comments\n",
    "# ASSUMING: from that month\n",
    "# ASSUMING: this is one 10 word span from each comment, not 5 for each comment\n",
    "# NOTE: not using this currently, mostly because using later spans actually might not be good \n",
    "# -- people tend to get more esoteric the longer they go one\n",
    "##################################################################\n",
    "def get_random_span(text, length):\n",
    "    text = [w for w in word_tokenize(text)]\n",
    "    try: \n",
    "        beg = random.randint(0, len(text) - length)\n",
    "        end = beg + 10\n",
    "        return text[beg:end]\n",
    "    except:\n",
    "        raise IndexError(\"Error: index out of range, probably happened if you didn't clean comments to be at least 10 words long\")\n",
    "        \n",
    "        \n",
    "# pulls 10 word spans from 50 top posts of a subreddit\n",
    "def get_top_posts(s, df_top_posts, num_posts=50):\n",
    "    df_top_posts_sub = df_top_posts[df_top_posts['subreddit_name'] == s]\n",
    "    return df_top_posts_sub.sample(num_posts, replace=True)['body'].apply(lambda x: word_tokenize(x)[:10]) \n",
    "\n",
    "# not using\n",
    "def get_guideline_text(s, df_subreddits, num_spans=50, text_col='cleaned'):\n",
    "    full_descr = list(df_subreddits[df_subreddits['subreddit_name'] == s][text_col])[0]\n",
    "    samples = []\n",
    "    for i in range(num_spans):\n",
    "        samples.append(get_random_span(full_descr, 10))\n",
    "    return samples\n",
    "\n",
    "##################################################################\n",
    "# get 50 comments by active users (10 comments from 5 randomly sampled active users, \n",
    "# who were not used to construct the SLM) and 50 comments from randomly-sampled outsiders\n",
    "# same length controlling effects used here - select 10 (text_len) word span from each comment\n",
    "##################################################################\n",
    "def get_user_comments(df, authors, month, num_posts, month_col='created_month', text_len=10):\n",
    "    if month is not None:\n",
    "        df = df[df[month_col] == int(month)]\n",
    "    else:\n",
    "        print('No month...taking all author counts')\n",
    "    df_author = df[df['author'].isin(authors)]\n",
    "    df_grouped = df_author.groupby('author')\n",
    "    sampled_comments = []\n",
    "    for a, g in df_grouped:\n",
    "        if num_posts:\n",
    "            if text_len is not None:\n",
    "                sample = g.sample(num_posts, replace=True)['body'].apply(lambda x: word_tokenize(x)[:text_len])\n",
    "            else: \n",
    "                sample = g.sample(num_posts, replace=True)['body'].apply(lambda x: word_tokenize(x))\n",
    "        else: \n",
    "            if text_len is not None:\n",
    "                sample = g['body'].apply(lambda x: word_tokenize(x)[:text_len]) \n",
    "            else: \n",
    "                sample = g['body'].apply(lambda x: word_tokenize(x)) \n",
    "        sampled_comments.extend(sample)\n",
    "    return random.choices(sampled_comments, k=num_posts*len(authors))\n",
    "\n",
    "\n",
    "# same as above but for the first user comment\n",
    "def get_user_first_comments(df, authors, month, month_col='created_month', total_num=250, text_len=10):\n",
    "    if month is not None:\n",
    "        df = df[df[month_col] == int(month)]\n",
    "    else:\n",
    "        print('No month...taking all author counts')\n",
    "    df_author = df[df['author'].isin(authors)]\n",
    "    df_grouped = df_author.groupby('author')\n",
    "    sampled_comments = []\n",
    "    for a, g in df_grouped:\n",
    "        if text_len is not None: \n",
    "            first = word_tokenize(g.sort_values(by='created_utc').iloc[0]['body'])[:text_len]\n",
    "        else:\n",
    "            first = word_tokenize(g.sort_values(by='created_utc').iloc[0]['body'])\n",
    "        sampled_comments.extend([first])\n",
    "    return random.choices(sampled_comments, k=total_num)\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# Importing data\n",
    "###################################################################\n",
    "def import_csvs(sub, path='data/cleaned/train/2017/', ext='_train_2017.csv', comment_pre_path='data/cleaned/sub_comments/', comment_ext='_comments_2017.csv'):\n",
    "    \n",
    "    # currently importing the same comments file for test/train\n",
    "    # This is because the authors have been seperated, so there shouldn't be any of the same messages\n",
    "    # between the two sets (even though they pull text from the same file)\n",
    "    comment_path = comment_pre_path+sub+comment_ext\n",
    "    \n",
    "    author_path = path+'author_counts/'+sub+'_author_counts'+ext\n",
    "    \n",
    "    print('Importing ', colored(comment_path, 'magenta'),'.....', end=' ')\n",
    "    df_sub_comments = pd.read_csv(comment_path, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    print('Done')\n",
    "    print('Importing ', colored(author_path, 'magenta'),'.....', end=' ')\n",
    "    df_author_counts = pd.read_csv(author_path, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    print('Done')\n",
    "    \n",
    "    # renaming month columns per the issue of having a string float\n",
    "    cols = df_author_counts.columns.tolist()\n",
    "    df_author_counts = df_author_counts.rename(index=str, columns={c:str(int(float(c))) for c in cols[2:len(cols)-1]})\n",
    "    \n",
    "    \n",
    "    return df_sub_comments, df_author_counts\n",
    "\n",
    "# simpler function for just importing a csv\n",
    "def import_csv(sub, path='data/cleaned/train/2017/', ext='_train_2017.csv', kind=None):\n",
    "\n",
    "    path = path+sub+ext\n",
    "    \n",
    "    print('Importing ', colored(path, 'magenta'),'.....', end=' ')\n",
    "    df = pd.read_csv(comment_path, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    print('Done')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_acc_gap(active_ent, outside_ent):\n",
    "    exp_val_active_ent = np.mean(active_ent)\n",
    "    exp_val_outside_ent = np.mean(outside_ent)\n",
    "    return (exp_val_outside_ent - exp_val_active_ent) / exp_val_active_ent\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/21532471/how-to-calculate-cohens-d-in-python\n",
    "def cohens_d(x, y):\n",
    "    return (x.mean() - y.mean()) / (math.sqrt((x.std() ** 2 + y.std() ** 2) / 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM building functions based on SRILM\n",
    "These have to stay in a notebook since it uses Jupyter magic to run the commands for SRILM. It also has mannnyy path dependencies, so make sure you know where 1) SRILM lives, 2) where you're store SRILM's lms, count files, and corperum, and 3) where this file exisits. Note that this file will change the directory to ...ARK/community_guidelines for simplicities sake\n",
    "\n",
    "These functions also require some of the functions from SLM_building.py in the lib folder, so also running that there. Though ideally in future iterations this will be taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export text for SRILM to train models on\n",
    "# takes the form of a list of strings, tokenized\n",
    "def export_text(text, name, corpus_path='../data/srilm_data/'):\n",
    "    text = [' '.join(t) for t in text]\n",
    "    pd.Series(text).to_csv(corpus_path + name + '.txt', sep='\\n', index=False, quoting=csv.QUOTE_NONE)\n",
    "    return corpus_path + name + '.txt'\n",
    "    \n",
    "    \n",
    "def train_SRILM(ngram_count_command, corpus, count_file, lm, vocab=None):\n",
    "    print('Reading text corpus at', colored(corpus, 'green'), ' and writing to count file ', colored(count_file, 'magenta'), '.....', end='')\n",
    "    if vocab is not None:\n",
    "        print('Training with vocab file...', colored(vocab, 'blue'), '...', end='')\n",
    "        ! {ngram_count_command} -text {corpus} -order 2 -write {count_file} -unk -vocab {vocab}\n",
    "    else:\n",
    "        ! {ngram_count_command} -text {corpus} -order 2 -write {count_file} -unk\n",
    "    print('Done')\n",
    "    print('Training LM from count file', colored(count_file, 'magenta'), ' to ', colored(lm, 'red'), '....', end='')\n",
    "    ! {ngram_count_command} -read {count_file} -order 2 -lm {lm} -gt1min 3 -gt1max 7 -gt2min 3 -gt2max 7\n",
    "    print('Done')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Calculate entropy for SRILM LM path\n",
    "# requrires Jupyter magic\n",
    "def get_SRILM_entropy(ngram_command, lm_path, test_text_path):\n",
    "    ppl_output = ! {ngram_command} -ppl {test_text_path} -lm {lm_path}\n",
    "    try:\n",
    "        ppl = float(ppl_output[1].split(' ')[5])\n",
    "    except IndexError:\n",
    "        print('Index out of range, probably due to there being no model where you pointed')\n",
    "        print('SRILM output: ', ppl_output)\n",
    "    return math.log(ppl,2)\n",
    "\n",
    "\n",
    "# full refers to if the LM uses the entire sentence or just the first 10 words\n",
    "def construct_LM_SRILM(active_user_text, sub_name, month, index_num, kind, vocab=None, year='2018', ngram_count_command='./../../../tools/SRILM/bin/i686-m64/ngram-count', full=False):\n",
    "    if month is not None:\n",
    "        month_str = str(month) \n",
    "    else:\n",
    "        month_str = 'total'\n",
    "    if vocab is not None:\n",
    "        vocab_str = vocab.replace('data/cleaned/', '').replace('.txt', '')\n",
    "    else:\n",
    "        vocab_str = ''\n",
    "    corpus_path = export_text(active_user_text, name=sub_name+'_'+str(index_num)+'_month_'+month_str)\n",
    "    if full:\n",
    "        count_file = '../srilms_LMs/counts/' + 'full_' + kind + '_' + sub_name +'_'+str(index_num)+'_month_'+month_str+'.count'\n",
    "        lm_path = '../srilms_LMs/' + year + '/alt_full_text_lms/' + sub_name+'_'+str(index_num)+'_month_'+month_str+'_'+vocab_str+'.lm'\n",
    "    else: \n",
    "        count_file = '../srilms_LMs/counts/' + kind + '_' + sub_name +'_'+str(index_num)+'_month_'+month_str+'.count'\n",
    "        lm_path = '../srilms_LMs/' + year + '/' + kind + '/' + sub_name + '/' +sub_name+'_'+str(index_num)+'_month_'+month_str+'_'+vocab_str+'.lm'\n",
    "    train_SRILM(ngram_count_command, corpus_path, count_file, lm_path, vocab=vocab) # if you want a vocab, you can add it here\n",
    "    return lm_path\n",
    "\n",
    "# TODO not needed, added new param above\n",
    "def construct_full_LM_SRILM(active_user_text, sub_name, month, index_num, kind, vocab=None, year='2018', ngram_count_command='./../../../tools/SRILM/bin/i686-m64/ngram-count'):\n",
    "    if month is not None:\n",
    "        month_str = str(month) \n",
    "    else:\n",
    "        month_str = 'total'\n",
    "    corpus_path = export_text(active_user_text, name=sub_name+'_'+str(index_num)+'_month_'+month_str)\n",
    "    count_file = '../srilms_LMs/counts/' + 'full_' + kind + '_' + sub_name +'_'+str(index_num)+'_month_'+month_str+'.count'\n",
    "    lm_path = '../srilms_LMs/' + year + '/alt_full_text_lms/' + sub_name+'_'+str(index_num)+'_month_'+month_str+'.lm'\n",
    "    train_SRILM(ngram_count_command, corpus_path, count_file, lm_path, vocab=None) # if you want a vocab, you can add it here\n",
    "    return lm_path\n",
    "\n",
    "# build an SLMs for a single month\n",
    "def build_SLMs_SRILM(df, author_counts, slm_count, month, name, num_authors, kind, threshold_count, year, full=False, vocab=None):\n",
    "    print('Creating ', colored(str(slm_count) + ' SLMs ', 'red'), 'for', colored(' month ' + str(month), 'green'), '.....')\n",
    "    slms = []\n",
    "    for i in range(0, slm_count): \n",
    "        active_users = get_active_users(author_counts, month, 'author', threshold=threshold_count, num_authors=num_authors, kind=kind)\n",
    "        if full:\n",
    "            active_user_comments = get_user_comments(df, list(active_users), month=month, num_posts=threshold_count, text_len=None)\n",
    "        else:\n",
    "            active_user_comments = get_user_comments(df, list(active_users), month=month, num_posts=threshold_count, text_len=10)\n",
    "        slm_path = construct_LM_SRILM(active_user_comments, name, month, i, kind=kind, year=year, full=full, vocab=vocab)\n",
    "        slms.append(slm_path)\n",
    "    return slms\n",
    "\n",
    "# build an SLMs for a single month\n",
    "# TODO not needed, added new param above\n",
    "def build_full_SLMs_SRILM(df, author_counts, slm_count, month, name, num_authors, kind, threshold_count, year):\n",
    "    print('Creating ', colored(str(slm_count) + ' Full SLMs ', 'red'), 'for', colored(' month ' + str(month), 'green'), '.....')\n",
    "    slms = []\n",
    "    for i in range(0, slm_count): \n",
    "        active_users = get_active_users(author_counts, month, 'author', threshold=threshold_count, num_authors=num_authors, kind=kind)\n",
    "        active_user_comments = get_user_comments(df, list(active_users), month=month, num_posts=threshold_count, text_len=None)\n",
    "        slm_path = construct_full_LM_SRILM(active_user_comments, name, month, i, kind=kind, year=year)\n",
    "        slms.append(slm_path)\n",
    "    return slms\n",
    "\n",
    "\n",
    "# returns dict of {month:SLM}\n",
    "def build_monthly_SLM_SRILM(df, author_counts, slm_count, name, use_saved_lms=False, kind=None, num_authors=200, threshold_count=5, year='2017', full=False, vocab=None):\n",
    "    slm_dict = {}\n",
    "    if vocab is not None:\n",
    "        vocab_str = '_'+vocab.replace('data/cleaned/', '').replace('.txt', '')\n",
    "    else:\n",
    "        vocab_str = ''\n",
    "    # if we don't want to remake the LMs, can just load our old ones\n",
    "    if use_saved_lms:\n",
    "        if full:\n",
    "            for m in MONTHS:\n",
    "                slms = []\n",
    "                for i in range(0, slm_count):\n",
    "                    slms.append('../srilms_LMs/'+ year + '/alt_full_text_lms/' + name +'_'+str(i)+'_month_'+str(m)+vocab_str+'.lm')\n",
    "                slm_dict[m] = slms\n",
    "            return slm_dict\n",
    "        else:\n",
    "            for m in MONTHS:\n",
    "                slms = []\n",
    "                for i in range(0, slm_count):\n",
    "                    slms.append('../srilms_LMs/'+ year +'/' + kind + '/' + name + '/' + name +'_'+str(i)+'_month_'+str(m)+vocab_str+'.lm')\n",
    "                slm_dict[m] = slms\n",
    "            return slm_dict\n",
    "    # otherwise just remake them\n",
    "    else: \n",
    "        for m in MONTHS:\n",
    "            slms = build_SLMs_SRILM(df, \n",
    "                                author_counts,\n",
    "                                slm_count,\n",
    "                                month=m,\n",
    "                                name=name,\n",
    "                                num_authors=num_authors, # num authors to sample\n",
    "                                kind=kind, # kind of text, either posts or comments\n",
    "                                threshold_count=threshold_count, # threshold of how many posts to use\n",
    "                                year=year,\n",
    "                                full=full,\n",
    "                                vocab=vocab)\n",
    "            slm_dict[m] = slms\n",
    "        return slm_dict\n",
    "\n",
    "# returns LM for entire df, not sampled by month\n",
    "def build_total_SLM_SRILM(df, author_counts, slm_count, name, use_saved_lms=False, kind=None, num_authors=200, threshold_count=5, year='2018', full=False, vocab=None):\n",
    "    slms = []\n",
    "    if vocab is not None:\n",
    "        vocab_str = '_'+vocab.replace('data/cleaned/', '').replace('.txt', '')\n",
    "    else:\n",
    "        vocab_str = ''\n",
    "    # if we don't want to remake the LMs, can just load our old ones\n",
    "    if use_saved_lms:\n",
    "        if full:\n",
    "            for i in range(0, slm_count):\n",
    "                slms.append('../srilms_LMs/'+ year + '/alt_full_text_lms/' + name +'_'+str(i)+'_month_total'+vocab_str+'.lm')\n",
    "            return slms\n",
    "        for i in range(0, slm_count):\n",
    "            slms.append('../srilms_LMs/'+ year + '/' + kind + '/' + name + '/' + name +'_'+str(i)+'_month_total'+vocab_str+'.lm')\n",
    "        return slms\n",
    "    # otherwise just remake them\n",
    "    else: \n",
    "        slms = build_SLMs_SRILM(df, \n",
    "                                author_counts,\n",
    "                                slm_count,\n",
    "                                month=None,\n",
    "                                name=name,\n",
    "                                num_authors=num_authors, # num authors to sample\n",
    "                                kind=kind, # kind of text, either posts or comments\n",
    "                                threshold_count=threshold_count, # threshold of how many posts to use\n",
    "                                year=year,\n",
    "                                full=full,\n",
    "                                vocab=vocab) \n",
    "        return slms\n",
    "    \n",
    "def calc_month_entropy_SRILM(slms, text_path):\n",
    "    entropies = []\n",
    "    for i, slm in enumerate(slms):\n",
    "        entropies.append(get_SRILM_entropy('/homes/gws/taugust/tools/SRILM/bin/i686-m64/ngram', slm, text_path))\n",
    "    return entropies\n",
    "\n",
    "\n",
    "    \n",
    "def calc_acc_gap_SRILM(slms, author_counts, comments, sub_name, kind=None, num_active_authors=10, num_active_posts=5, num_outside_authors=50, num_outside_posts=1, first=False):\n",
    "    monthly_acc_gap = {}\n",
    "    entropies = {'inside':[], 'outside':[]}\n",
    "    for month in slms.keys():\n",
    "        acc_gap, (active_ent, outside_ent) = calc_single_acc_gap_SRILM(slms[month], \n",
    "                author_counts, comments, sub_name, month=month, kind=kind,\n",
    "                num_active_authors=num_active_authors, num_active_posts=num_active_posts,\n",
    "                num_outside_authors=num_outside_authors, num_outside_posts=num_outside_posts, first=first)\n",
    "        \n",
    "        monthly_acc_gap[month] = acc_gap\n",
    "        entropies['inside'].append(active_ent)\n",
    "        entropies['outside'].append(outside_ent)\n",
    "        \n",
    "        print('Saving acc gap for', colored('month ' + str(month), 'green'))\n",
    "    return monthly_acc_gap, entropies\n",
    "\n",
    "\n",
    "# returns difference of the means of the cross entropy of outside vs. inside text divided by inside posts\n",
    "def calc_single_acc_gap_SRILM(slms, author_counts, comments, sub_name, month=None, kind=None, num_active_authors=10, num_active_posts=5, num_outside_authors=50, num_outside_posts=1, active_threshold=5, first=False):\n",
    "    if month is not None:\n",
    "        month_string = str(month)\n",
    "    else:\n",
    "        month_string = 'total'\n",
    "    print('Calculating cross entropy for', colored('month ' + month_string, 'green'), '.....')\n",
    "    \n",
    "    if first:\n",
    "        active_authors = get_first_active_users(author_counts, month, 'author', threshold=active_threshold, num_authors=num_active_authors, kind=kind)\n",
    "        active_comments = get_user_first_comments(comments, list(active_authors), month=month)\n",
    "    else: \n",
    "        active_authors = get_active_users(author_counts, month, 'author', threshold=active_threshold, num_authors=num_active_authors, kind=kind)\n",
    "        active_comments = get_user_comments(comments, list(active_authors), month=month, num_posts=num_active_posts)\n",
    "\n",
    "    outside_authors = get_outside_users(author_counts, month, 'author', threshold=1, num_authors=num_outside_authors, kind=kind)\n",
    "    print('sampled active users:', len(active_authors), 'sampled outside users:', len(outside_authors))\n",
    "\n",
    "    \n",
    "    outside_comments = get_user_comments(comments, list(outside_authors), month=month, num_posts=num_outside_posts)\n",
    "    \n",
    "#     active_comments = get_user_first_comments(comments, list(active_authors), month=month, total_num=50)\n",
    "#     outside_comments = get_user_comments(comments, list(outside_authors), month=month, num_posts=num_outside_posts)\n",
    "    \n",
    "    print('sampled active comments:', len(active_comments), 'sampled outside comments:', len(outside_comments))\n",
    "\n",
    "    # export to SRILM test directory\n",
    "    active_corpus_file = export_text(active_comments, name=sub_name+'_inside_month_'+month_string, corpus_path='../data/srilm_data/test_')\n",
    "    outside_corpus_file = export_text(outside_comments, name=sub_name+'_outside_month_'+month_string, corpus_path='../data/srilm_data/test_')\n",
    "\n",
    "    # run through SRILM LMs for the month and get the entropy\n",
    "    active_ent = calc_month_entropy_SRILM(slms, active_corpus_file)\n",
    "    outside_ent = calc_month_entropy_SRILM(slms, outside_corpus_file)\n",
    "    \n",
    "    # calculate the acc gap\n",
    "    exp_val_active_ent = np.mean(active_ent)\n",
    "    exp_val_outside_ent = np.mean(outside_ent)\n",
    "\n",
    "    acc_gap = (exp_val_outside_ent - exp_val_active_ent) / exp_val_active_ent\n",
    "\n",
    "    \n",
    "    return acc_gap, (active_ent, outside_ent)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility funcs for calculating acc gaps\n",
    "\n",
    "These functions are pretty weird, just super specialized to what I was doing, honestly using the above smaller functions are probably a safer bet if you can't remember exactly what these are for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These functions a\n",
    "def get_df_posts_and_comments(s, year):\n",
    "    df_comments, df_author_counts_test = import_csvs(s, path='data/cleaned/test/'+year+'/', ext='_test_'+year+'.csv', comment_pre_path='data/cleaned/sub_comments/', comment_ext='_comments_'+year+'.csv')    \n",
    "    df_posts, df_author_counts_test = import_csvs(s, path='data/cleaned/test/'+year+'/', ext='_test_'+year+'.csv', comment_pre_path='data/cleaned/sub_posts/', comment_ext='_posts_'+year+'.csv')\n",
    "    df_posts = df_posts.rename(index=str, columns={'fulltext': 'body'})\n",
    "    return df_comments, df_posts, df_author_counts_test\n",
    "    \n",
    "\n",
    "\n",
    "# Func for calculating acculuration gap of a passed subreddit for comments and posts based on POST-BASED LMs\n",
    "# Since post based LMs are not month specific, this function samples 12 times for the same LM to get the same\n",
    "# amount of observations as the comment based SLMs that sample by month\n",
    "\n",
    "# The return value is also a little odd:\n",
    "### total_acc_gap_comments = acc gap of comments (based on posts)\n",
    "### entropies_comments =  all inside and outside entropies of comments, used to calculate total_acc_gap_comments\n",
    "### total_acc_gap_posts = acc gap of posts\n",
    "### entropies_posts = all inside and outside entropies of posts, used to calculate total_acc_gap_posts\n",
    "def get_acc_gap_post(post_slms, s, year='2018'):\n",
    "    df_comments, df_posts, df_author_counts_test = get_df_posts_and_comments(s, year)\n",
    "    total_acc_gap_comments = []\n",
    "    entropies_comments = {'inside':[], 'outside':[]}\n",
    "    total_acc_gap_posts = []\n",
    "    entropies_posts = {'inside':[], 'outside':[]}\n",
    "    for i in range(1, 13):\n",
    "        # Comments #\n",
    "        ############\n",
    "        gap_comment, (active_ent_comment, outside_ent_comment) = calc_single_acc_gap_SRILM(post_slms, df_author_counts_test, df_comments, s, kind='comment', month=None,\n",
    "                                           num_active_authors=10, num_active_posts=5, num_outside_authors=50, num_outside_posts=1)\n",
    "        total_acc_gap_comments.append(gap_comment)\n",
    "        entropies_comments['inside'].append(active_ent_comment)\n",
    "        entropies_comments['outside'].append(outside_ent_comment)\n",
    "    \n",
    "        # Posts #\n",
    "        ############\n",
    "        gap_post, (active_ent_post, outside_ent_post) = calc_single_acc_gap_SRILM(post_slms, df_author_counts_test, df_posts, s, kind='post', month=None, \n",
    "                                            num_active_authors=10, num_active_posts=5, num_outside_authors=50, num_outside_posts=1)\n",
    "        total_acc_gap_posts.append(gap_post)\n",
    "        entropies_posts['inside'].append(active_ent_post)\n",
    "        entropies_posts['outside'].append(outside_ent_post)\n",
    "        \n",
    "    return (total_acc_gap_comments,entropies_comments), (total_acc_gap_posts, entropies_posts)\n",
    "\n",
    "# Same function but for comment-based slms\n",
    "def get_acc_gap_comment(comment_slms, s, year='2018'):\n",
    "    df_comments, df_posts, df_author_counts_test = get_df_posts_and_comments(s, year)\n",
    "    \n",
    "    # Comments #\n",
    "    ############\n",
    "    # Just use the full acc gap function to loop through all the months \n",
    "    print('Getting avg acc gap for', colored('comments', 'green'), 'with passed slms')\n",
    "    dict_gap_comments, entropies_comments = calc_acc_gap_SRILM(comment_slms, df_author_counts_test, df_comments, s, kind='comment',\n",
    "                                       num_active_authors=10, num_active_posts=5, num_outside_authors=50, num_outside_posts=1)\n",
    "    gap_comments = list(dict_gap_comments.values())\n",
    "    \n",
    "    # Posts #\n",
    "    #########\n",
    "    # Here again just loop through the SLMs and sample from the full population each time\n",
    "    print('Getting average acc gap for', colored('posts', 'green'), 'with passed slms')\n",
    "    total_acc_gap_posts = []\n",
    "    entropies_posts = {'inside':[], 'outside':[]}\n",
    "    for month in comment_slms.keys():\n",
    "        gap_posts, (active_ent_posts, outside_ent_posts) = calc_single_acc_gap_SRILM(comment_slms[month], \n",
    "                df_author_counts_test, df_posts, s, kind='post', month=None, \n",
    "                num_active_authors=10, num_active_posts=5, num_outside_authors=50, num_outside_posts=1, active_threshold=5)\n",
    "        total_acc_gap_posts.append(gap_posts)\n",
    "        entropies_posts['inside'].append(active_ent_posts)\n",
    "        entropies_posts['outside'].append(outside_ent_posts)\n",
    "\n",
    "    return (gap_comments,entropies_comments), (total_acc_gap_posts, entropies_posts)\n",
    "\n",
    "\n",
    "\n",
    "# specizlied function for flattening entropies, because they are just so damn nested\n",
    "def flatten_entropies(entropy_posts, entropy_comments):\n",
    "    # flatten these lists -- treating them each as an observation\n",
    "    entropy_comments['inside'] = list(flatten(entropy_comments['inside']))\n",
    "    entropy_comments['outside'] = list(flatten(entropy_comments['outside']))\n",
    "\n",
    "    entropy_posts['inside'] = list(flatten(entropy_posts['inside']))\n",
    "    entropy_posts['outside'] = list(flatten(entropy_posts['outside']))\n",
    "    \n",
    "    return entropy_posts, entropy_comments\n",
    "\n",
    "# function for grouping inside and outside entropies,\n",
    "# returns dfs grouped by post/comment - outside in one and inside in the other\n",
    "def group_inside_outside(entropy_posts, entropy_comments):\n",
    "    # group inside posts and comments and outside and outside\n",
    "    inside_post_and_comments = {'posts':entropy_posts['inside'], 'comments':entropy_comments['inside']}\n",
    "    outside_post_and_comments = {'posts':entropy_posts['outside'], 'comments':entropy_comments['outside']}\n",
    "\n",
    "    # convert this into a df to plot more easily\n",
    "    df_inside_post_and_comments = pd.DataFrame(inside_post_and_comments)\n",
    "    df_outside_post_and_comments = pd.DataFrame(outside_post_and_comments)\n",
    "    \n",
    "    return df_inside_post_and_comments, df_outside_post_and_comments\n",
    "\n",
    "\n",
    "# function for grouping comment and post entropies,\n",
    "# returns dfs grouped by post/comment \n",
    "def group_comments_posts(entropy_posts, entropy_comments):\n",
    "    # group inside posts and comments and outside and outside\n",
    "    posts = {'inside':entropy_posts['inside'], 'outside':entropy_posts['outside']}\n",
    "    comments = {'inside':entropy_comments['inside'], 'outside':entropy_comments['outside']}\n",
    "\n",
    "    # convert this into a df to plot more easily\n",
    "    df_posts_inside_outside = pd.DataFrame(posts)\n",
    "    df_comments_inside_outside = pd.DataFrame(comments)\n",
    "    \n",
    "    return df_posts_inside_outside, df_comments_inside_outside\n",
    "\n",
    "\n",
    "\n",
    "def plot_entropy_dist(df, labels, ax, title):\n",
    "    for l in labels:\n",
    "        sns.distplot(df[l], ax=ax, label=l)\n",
    "    ax.set_title(title, fontsize=40)\n",
    "    ax.set_xlabel('cross entropy', fontsize=25)\n",
    "    ax.legend(fontsize=25)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=35)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anova(values, non_param):\n",
    "    if non_param: \n",
    "        f, p = stats.kruskal(*values)\n",
    "    else: \n",
    "        f, p = stats.f_oneway(*values)\n",
    "    # degrees of freedom for ANOVA\n",
    "    anova_btwn = len(values) - 1\n",
    "    anova_wthn = (len([val for sublist in values for val in sublist]) - (anova_btwn + 1))\n",
    "    print('F ( ', anova_btwn, ', ', anova_wthn, ') =', ('%.3f' % f), ' p =', ('%.10f' % p))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "reddit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
