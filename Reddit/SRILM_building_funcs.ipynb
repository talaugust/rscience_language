{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/taugust/ARK/community_guidelines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import seaborn as sns\n",
    "import uuid\n",
    "\n",
    "\n",
    "from nltk.util import bigrams, ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten, padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "os.chdir('/homes/gws/taugust/ARK/community_guidelines/')\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# redefining alt functions that don't rely on month\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for SLM building \n",
    "Take fron lib/SLM_building.py, added here so all the functions for SLM building could be in the same place\n",
    "the lib/ file is in the process of being phased out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = list(range(1,13))\n",
    "\n",
    "##################################################################\n",
    "# randomly sampling 200 users  \n",
    "# defined as users with at least 5 in the respective community and month.\n",
    "# Kind defines if you are looking at comments or posts, if None then using both\n",
    "\n",
    "# TODO: might not want to reset the author_df, maybe make a copy?\n",
    "##################################################################\n",
    "# def get_active_users(author_df, month, author_col, threshold=5, num_authors=200, kind=None):\n",
    "#     if kind:\n",
    "#         author_df = author_df[author_df['kind'] == kind]\n",
    "#     if num_authors: \n",
    "#         print(len(author_df[author_df[month] > threshold]))\n",
    "#         return author_df[author_df[month] > threshold].drop_duplicates().sample(num_authors)[author_col]\n",
    "#     else: \n",
    "#         return author_df[author_df[month] > threshold].drop_duplicates()[author_col]\n",
    "\n",
    "\n",
    "# if no month, assume you want the entire df and set threshold for total counts\n",
    "def get_active_users(author_df, month, author_col, threshold=5, num_authors=200, kind=None):\n",
    "    if kind:\n",
    "        author_df = author_df[author_df['kind'] == kind]  \n",
    "    if month is not None:\n",
    "        author_df = author_df[author_df[str(month)] > threshold].drop_duplicates()\n",
    "    else:\n",
    "        print('No month...taking all author counts')\n",
    "        author_df = author_df[author_df[[str(m) for m in MONTHS]].sum(axis=1) >= threshold]\n",
    "    if num_authors: \n",
    "        return author_df.sample(num_authors)[author_col]\n",
    "    return author_df[author_col]\n",
    "    \n",
    "##################################################################\n",
    "# getting outsiders -- users who only ever posted once in a community-- \n",
    "# but are still activein Reddit in general  (TODO)\n",
    "##################################################################\n",
    "# def get_outside_users(author_df, month, author_col, threshold=1, num_authors=None, kind=None):\n",
    "#     if kind:\n",
    "#         author_df = author_df[author_df['kind'] == kind]\n",
    "#     outside_users = author_df[author_df[[str(m) for m in MONTHS]].sum(axis=1) <= threshold]\n",
    "#     if num_authors:\n",
    "#         return outside_users[outside_users[month] == threshold][author_col].sample(num_authors)\n",
    "#     else: \n",
    "#         return outside_users[outside_users[month] == threshold][author_col]\n",
    "\n",
    "def get_outside_users(author_df, month, author_col, threshold=1, num_authors=None, kind=None):\n",
    "    if kind:\n",
    "        author_df = author_df[author_df['kind'] == kind]\n",
    "    outside_users = author_df[author_df[[str(m) for m in MONTHS]].sum(axis=1) <= threshold]\n",
    "    if month is not None:\n",
    "        outside_users = outside_users[outside_users[str(month)] == threshold]\n",
    "    else:\n",
    "        print('No month...taking all author counts')    \n",
    "    if num_authors:\n",
    "        return outside_users[author_col].sample(num_authors)\n",
    "    return outside_users[author_col]\n",
    "    \n",
    "##################################################################   \n",
    "# For each of these 200 active users\n",
    "# select 5 random 10-word spans from 5 unique comments\n",
    "# ASSUMING: from that month\n",
    "# ASSUMING: this is one 10 word span from each comment, not 5 for each comment\n",
    "# NOTE: Note using this currently, mostly because using later spans actually might not be good \n",
    "# -- people tend to get more esoteric the longer they go one\n",
    "##################################################################\n",
    "def get_random_span(text, length):\n",
    "    text = [w for w in word_tokenize(text)]\n",
    "    try: \n",
    "        beg = random.randint(0, len(text) - length)\n",
    "        end = beg + 10\n",
    "        return text[beg:end]\n",
    "    except:\n",
    "        raise IndexError(\"Error: index out of range, probably happened if you didn't clean comments to be at least 10 words long\")\n",
    "        \n",
    "        \n",
    "##################################################################\n",
    "# get 50 comments by active users (10 comments from 5 randomly sampled active users, \n",
    "# who were not used to construct the SLM -TODO) and 50 comments from randomly-sampled outsiders\n",
    "# same length controlling effects used here - select random 10 word span from each comment (see ASSUMING above)\n",
    "##################################################################\n",
    "# def get_user_comments(df, authors, month, num_posts, month_col='created_month'):\n",
    "#     df_month = df[df[month_col] == month]\n",
    "#     df_month_author = df_month[df_month['author'].apply(lambda x: x in authors)]\n",
    "#     df_grouped = df_month_author.groupby('author')\n",
    "#     sampled_comments = []\n",
    "#     for a, g in df_grouped:\n",
    "#         if num_posts:\n",
    "#             sample = g.sample(num_posts)['body'].apply(lambda x: [w for w in word_tokenize(x)][:10]) \n",
    "#         else: \n",
    "#             sample = g['body'].apply(lambda x: [w for w in word_tokenize(x)][:10]) \n",
    "#         sampled_comments.extend(sample)\n",
    "#     return sampled_comments\n",
    "\n",
    "def get_user_comments(df, authors, month, num_posts, month_col='created_month'):\n",
    "    if month is not None:\n",
    "        df = df[df[month_col] == int(month)]\n",
    "    else:\n",
    "        print('No month...taking all author counts')\n",
    "    df_author = df[df['author'].apply(lambda x: x in authors)]\n",
    "    df_grouped = df_author.groupby('author')\n",
    "    sampled_comments = []\n",
    "    for a, g in df_grouped:\n",
    "        if num_posts:\n",
    "            sample = g.sample(num_posts)['body'].apply(lambda x: [w for w in word_tokenize(x)][:10]) \n",
    "        else: \n",
    "            sample = g['body'].apply(lambda x: [w for w in word_tokenize(x)][:10]) \n",
    "        sampled_comments.extend(sample)\n",
    "    return sampled_comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# Importing data\n",
    "###################################################################\n",
    "def import_csvs(sub, path='data/cleaned/train/2017/', ext='_train_2017.csv', comment_pre_path='data/cleaned/sub_comments/', comment_ext='_comments_2017.csv'):\n",
    "    \n",
    "    # currently importing the same comments file for test/train\n",
    "    # This is because the authors have been seperated, so there shouldn't be any of the same messages\n",
    "    # between the two sets (even though they pull text from the same file)\n",
    "    comment_path = comment_pre_path+sub+comment_ext\n",
    "    \n",
    "    author_path = path+'author_counts/'+sub+'_author_counts'+ext\n",
    "    \n",
    "    print('Importing ', colored(comment_path, 'magenta'),'.....', end=' ')\n",
    "    df_sub_comments = pd.read_csv(comment_path, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    print('Done')\n",
    "    print('Importing ', colored(author_path, 'magenta'),'.....', end=' ')\n",
    "    df_author_counts = pd.read_csv(author_path, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    print('Done')\n",
    "    \n",
    "    # renaming month columns per the issue of having a string float\n",
    "    cols = df_author_counts.columns.tolist()\n",
    "    df_author_counts = df_author_counts.rename(index=str, columns={c:str(int(float(c))) for c in cols[2:len(cols)-1]})\n",
    "    \n",
    "    \n",
    "    return df_sub_comments, df_author_counts\n",
    "\n",
    "# simpler function for just importing a csv\n",
    "def import_csv(sub, path='data/cleaned/train/2017/', ext='_train_2017.csv', kind=None):\n",
    "\n",
    "    path = path+sub+ext\n",
    "    \n",
    "    print('Importing ', colored(path, 'magenta'),'.....', end=' ')\n",
    "    df = pd.read_csv(comment_path, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    print('Done')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM building functions based on SRILM\n",
    "These have to stay in a notebook since it uses Jupyter magic to run the commands for SRILM. It also has mannnyy path dependencies, so make sure you know where 1) SRILM lives, 2) where you're store SRILM's lms, count files, and corperum, and 3) where this file exisits. Note that this file will change the directory to ...ARK/community_guidelines for simplicities sake\n",
    "\n",
    "These functions also require some of the functions from SLM_building.py in the lib folder, so also running that there. Though ideally in future iterations this will be taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change to absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# export text for SRILM to train models on\n",
    "# takes the form of a list of strings, tokenized\n",
    "def export_text(text, name, corpus_path='../data/srilm_data/'):\n",
    "    pd.Series(text).to_csv(corpus_path + name + '.txt', sep='\\n', index=False, quoting=csv.QUOTE_NONE)\n",
    "    return corpus_path + name + '.txt'\n",
    "    \n",
    "    \n",
    "###################################################################\n",
    "def train_SRILM(ngram_count_command, corpus, count_file, lm):\n",
    "    print('Reading text corpus at', colored(corpus, 'green'), ' and writing to count file ', colored(count_file, 'magenta'), '.....', end='')\n",
    "    ! {ngram_count_command} -text {corpus} -order 2 -write {count_file} -unk \n",
    "    print('Done')\n",
    "    print('Training LM from count file', colored(count_file, 'magenta'), ' to ', colored(lm, 'red'), '....', end='')\n",
    "    ! {ngram_count_command} -read {count_file} -order 2 -lm {lm} -gt1min 3 -gt1max 7 -gt2min 3 -gt2max 7\n",
    "    print('Done')\n",
    "    \n",
    "    \n",
    "###################################################################    \n",
    "# Calculate entropy for SRILM LM path\n",
    "# requrires Jupyter magic\n",
    "def get_SRILM_entropy(ngram_command, lm_path, test_text_path):\n",
    "    ppl_output = ! {ngram_command} -ppl {test_text_path} -lm {lm_path}\n",
    "    try:\n",
    "        ppl = float(ppl_output[1].split(' ')[5])\n",
    "    except IndexError:\n",
    "        print('Index out of range, probably due to there being no model where you pointed')\n",
    "        print('SRILM output: ', ppl_output)\n",
    "    return math.log(ppl,2)\n",
    "\n",
    "###################################################################\n",
    "# def construct_LM_SRILM(active_user_text, sub_name, month, index_num, kind, vocab=None, ngram_count_command='./../../tools/SRILM/bin/i686-m64/ngram-count'):\n",
    "#     corpus_path = export_text(active_user_text, name=sub_name+'_'+str(index_num)+'_month_'+str(month))\n",
    "    \n",
    "#     count_file = '../srilms_LMs/counts/' + kind + '_' + sub_name +'_'+str(index_num)+'_month_'+str(month)+'.count'\n",
    "#     lm_path = '../srilms_LMs/' + kind + '/' + sub_name + '/' +sub_name+'_'+str(index_num)+'_month_'+str(month)+'.lm'\n",
    "    \n",
    "#     train_SRILM(ngram_count_command, corpus_path, count_file, lm_path)\n",
    "#     return lm_path\n",
    "\n",
    "# CHANGED TO 2018\n",
    "def construct_LM_SRILM(active_user_text, sub_name, month, index_num, kind, vocab=None, ngram_count_command='./../../tools/SRILM/bin/i686-m64/ngram-count'):\n",
    "    if month is not None:\n",
    "        month_str = str(month) \n",
    "    else:\n",
    "        month_str = 'total'\n",
    "    corpus_path = export_text(active_user_text, name=sub_name+'_'+str(index_num)+'_month_'+month_str)\n",
    "    count_file = '../srilms_LMs/counts/2018/' + kind + '_' + sub_name +'_'+str(index_num)+'_month_'+month_str+'.count'\n",
    "    lm_path = '../srilms_LMs/2018/' + kind + '/' + sub_name + '/' +sub_name+'_'+str(index_num)+'_month_'+month_str+'.lm'\n",
    "    train_SRILM(ngram_count_command, corpus_path, count_file, lm_path)\n",
    "    return lm_path\n",
    "\n",
    "###################################################################\n",
    "# build an SLMs for a single month\n",
    "def build_SLMs_SRILM(df, author_counts, slm_count, month, name, num_authors, kind, threshold_count):\n",
    "    print('Creating ', colored(str(slm_count) + ' SLMs ', 'red'), 'for', colored(' month ' + str(month), 'green'), '.....')\n",
    "    slms = []\n",
    "    for i in range(0, slm_count): \n",
    "        active_users = get_active_users(author_counts, month, 'author', threshold=threshold_count, num_authors=num_authors, kind=kind)\n",
    "        active_user_comments = get_user_comments(df, list(active_users), month=month, num_posts=threshold_count)\n",
    "        slm_path = construct_LM_SRILM(active_user_comments, name, month, i, kind=kind)\n",
    "        slms.append(slm_path)\n",
    "    return slms\n",
    "\n",
    "\n",
    "# returns dict of {month:SLM}\n",
    "def build_monthly_SLM_SRILM(df, author_counts, slm_count, name, use_saved_lms=False, kind=None, num_authors=200, threshold_count=5):\n",
    "    slm_dict = {}\n",
    "    # if we don't want to remake the LMs, can just load our old ones\n",
    "    if use_saved_lms:\n",
    "        for m in MONTHS:\n",
    "            slms = []\n",
    "            for i in range(0, slm_count):\n",
    "                slms.append('../srilms_LMs/'+ kind + '/' + name + '/' + name +'_'+str(i)+'_month_'+str(m)+'.lm')\n",
    "            slm_dict[m] = slms\n",
    "        return slm_dict\n",
    "    # otherwise just remake them\n",
    "    else: \n",
    "        for m in MONTHS:\n",
    "            slms = build_SLMs_SRILM(df, \n",
    "                                    author_counts,\n",
    "                                    slm_count,\n",
    "                                    month=m,\n",
    "                                    name=name,\n",
    "                                    num_authors=num_authors, # num authors to sample\n",
    "                                    kind=kind, # kind of text, either posts or comments\n",
    "                                    threshold_count=threshold_count) # threshold of how many posts to use\n",
    "            slm_dict[m] = slms\n",
    "        return slm_dict\n",
    "\n",
    "###################################################################\n",
    "# returns LM for entire df, not sampled by month\n",
    "def build_total_SLM_SRILM(df, author_counts, slm_count, name, use_saved_lms=False, kind=None, num_authors=200, threshold_count=5):\n",
    "    slms = []\n",
    "    # if we don't want to remake the LMs, can just load our old ones\n",
    "    if use_saved_lms:\n",
    "        for i in range(0, slm_count):\n",
    "            slms.append('../srilms_LMs/'+ kind + '/' + name + '/' + name +'_'+str(i)+'_month_total.lm')\n",
    "        return slms\n",
    "    # otherwise just remake them\n",
    "    else: \n",
    "        slms = build_SLMs_SRILM(df, \n",
    "                                author_counts,\n",
    "                                slm_count,\n",
    "                                month=None,\n",
    "                                name=name,\n",
    "                                num_authors=num_authors, # num authors to sample\n",
    "                                kind=kind, # kind of text, either posts or comments\n",
    "                                threshold_count=threshold_count) # threshold of how many posts to use\n",
    "        return slms\n",
    "    \n",
    "###################################################################  \n",
    "def calc_month_entropy_SRILM(slms, text_path):\n",
    "    entropies = []\n",
    "    for i, slm in enumerate(slms):\n",
    "        # pretty sure this works since SRILM takes each line in the text file as seperate and outputs an average \n",
    "        entropies.append(get_SRILM_entropy('/homes/gws/taugust/tools/SRILM/bin/i686-m64/ngram', slm, text_path))\n",
    "    return entropies\n",
    "\n",
    "\n",
    "###################################################################    \n",
    "def calc_acc_gap_SRILM(slms, author_counts, comments, sub_name, kind=None, num_active_authors=5, num_active_posts=10, num_outside_authors=50, num_outside_posts=1):\n",
    "    monthly_acc_gap = {}\n",
    "    entropies = {'inside':[], 'outside':[]}\n",
    "    for month in slms.keys():\n",
    "        acc_gap, (active_ent, outside_ent) = calc_single_acc_gap_SRILM(slms[month], \n",
    "                author_counts, comments, sub_name, month=month, kind=kind,\n",
    "                num_active_authors=num_active_authors, num_active_posts=num_active_posts,\n",
    "                num_outside_authors=num_outside_authors, num_outside_posts=num_outside_posts)\n",
    "        \n",
    "        monthly_acc_gap[month] = acc_gap\n",
    "        entropies['inside'].append(active_ent)\n",
    "        entropies['outside'].append(outside_ent)\n",
    "        \n",
    "        print('Saving acc gap for', colored('month ' + str(month), 'green'))\n",
    "    return monthly_acc_gap, entropies\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# TODO: merge with above\n",
    "# returns difference of the means of the cross entropy of outside vs. inside text divided by inside posts\n",
    "def calc_single_acc_gap_SRILM(slms, author_counts, comments, sub_name, month=None, kind=None, num_active_authors=5, num_active_posts=10, num_outside_authors=50, num_outside_posts=1):\n",
    "    if month is not None:\n",
    "        month_string = str(month)\n",
    "    else:\n",
    "        month_string = 'total'\n",
    "    print('Calculating cross entropy for', colored('month ' + month_string, 'green'), '.....')\n",
    "    active_authors = get_active_users(author_counts, month, 'author', threshold=10, num_authors=num_active_authors, kind=kind)\n",
    "    outside_authors = get_outside_users(author_counts, month, 'author', threshold=1, num_authors=num_outside_authors, kind=kind)\n",
    "    print('sampled active users:', len(active_authors), 'sampled outside users:', len(outside_authors))\n",
    "\n",
    "    active_comments = get_user_comments(comments, list(active_authors), month=month, num_posts=num_active_posts)\n",
    "    outside_comments = get_user_comments(comments, list(outside_authors), month=month, num_posts=num_outside_posts)\n",
    "    \n",
    "    print('sampled active comments:', len(active_comments), 'sampled outside comments:', len(outside_comments))\n",
    "\n",
    "    # export to SRILM test directory\n",
    "    active_corpus_file = export_text(active_comments, name=sub_name+'_inside_month_'+month_string, corpus_path='../data/srilm_data/test')\n",
    "    outside_corpus_file = export_text(outside_comments, name=sub_name+'_outside_month_'+month_string, corpus_path='../data/srilm_data/test')\n",
    "\n",
    "    # run through SRILM LMs for the month and get the entropy\n",
    "    active_ent = calc_month_entropy_SRILM(slms, active_corpus_file)\n",
    "    outside_ent = calc_month_entropy_SRILM(slms, outside_corpus_file)\n",
    "    \n",
    "    # calculate the acc gap\n",
    "    exp_val_active_ent = np.mean(active_ent)\n",
    "    exp_val_outside_ent = np.mean(outside_ent)\n",
    "\n",
    "    acc_gap = (exp_val_outside_ent - exp_val_active_ent) / exp_val_active_ent\n",
    "\n",
    "    # also save variance of the acc gap by subtracting: https://www.kean.edu/~fosborne/bstat/05b2means.html\n",
    "#     var_active_ent = np.var(active_ent)\n",
    "#     var_outside_ent = np.var(outside_ent)\n",
    "\n",
    "#     acc_gap_var = (var_active_ent/len(active_ent)) + (var_outside_ent/len(outside_ent))\n",
    "    \n",
    "    return acc_gap, (active_ent, outside_ent)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "reddit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
