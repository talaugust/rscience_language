{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier to see how easy it is to identify r/science based on posts and comments\n",
    "\n",
    "### Will use posts/comments seperately\n",
    "### One vs. all classification with r/science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import os \n",
    "os.chdir('/homes/gws/taugust/Projects/ARK/community_guidelines/')\n",
    "\n",
    "all_posts_dir = 'data/cleaned/full_real_subs_cleaned_posts_2018_short.csv'\n",
    "all_comments_dir = 'data/cleaned/full_real_subs_cleaned_comments_2018_short.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a quick and simple tokenizer\n",
    "# (FWIW: I'm pretty sure I created this for something else, it's not perfect but ...\n",
    "# ... the point is to remove punctuation somewhat sensibly, lower case, and split)\n",
    "\n",
    "punct_chars = list(set(string.punctuation) - set(\"'\"))\n",
    "punct_chars.sort()\n",
    "punctuation = ''.join(punct_chars)\n",
    "replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "def text_to_tokens(text, lower=True, ngram=None):\n",
    "    # replace underscores with spaces\n",
    "    text = re.sub(r'_', ' ', text)\n",
    "    # break off single quotes at the ends of words (e.g. 'test' -> test)\n",
    "    text = re.sub(r'\\s\\'', ' ', text)\n",
    "    text = re.sub(r'\\'\\s', ' ', text)\n",
    "    # remove periods (e.g. U.S. -> US)\n",
    "    text = re.sub(r'\\.', '', text)\n",
    "    # replace all other punctuation (except single quotes) with spaces (e.g. T-rex -> t rex)\n",
    "    text = replace.sub(' ', text)\n",
    "    # remove single quotes (e.g. don't -> dont)\n",
    "    text = re.sub(r'\\'', '', text)\n",
    "    # replace all whitespace with a single space\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    # strip off spaces on either end\n",
    "    text = text.strip()    \n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    split_text = text.split()\n",
    "    if ngram is None:\n",
    "        return split_text\n",
    "    else:\n",
    "        return [tuple(split_text[i:i+ngram]) for i in range(len(split_text)-ngram+1)]\n",
    "    \n",
    "# convert list for bigrams to tuple\n",
    "def convert_to_tuple(line, cols):\n",
    "    for col in cols:\n",
    "        line[col] = [tuple(bigram) for bigram in line[col]]\n",
    "    return line\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline for classifier\n",
    "text_clf_pipeline = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer(ngram_range=(1, 2, ), stop_words=None, smooth_idf=False, max_features=1000)),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__max_df': [0.25, 0.5, 0.75, 1.0],\n",
    "              'tfidf__max_features': [10, 50, 100, 250, 500, 1000, None],\n",
    "              'tfidf__stop_words': ('english', None),\n",
    "              'tfidf__smooth_idf': (True, False),\n",
    "              'tfidf__norm': ('l1', 'l2', None),\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all data from json file that the frequency tests use\n",
    "with open('data/cleaned/all_posts_2018.jsonlist') as f:\n",
    "    posts = f.readlines()\n",
    "posts = [json.loads(line) for line in posts] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.DataFrame.from_records(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115051\n"
     ]
    }
   ],
   "source": [
    "posts_df['full_text'] = posts_df['title'] + ' ' + posts_df['selftext'] \n",
    "print(len(posts_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    92894\n",
      "True     22157\n",
      "Name: is_science, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "posts_df['is_science'] = posts_df['subreddit'] == 'science'\n",
    "print(posts_df['is_science'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_sampled_not_science = posts_df[~posts_df['is_science']].sample(len(posts_df[posts_df['is_science']]), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_sampled_balanced = posts_df_sampled_not_science.append(posts_df[posts_df['is_science']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a balanced subset of the data to train and test on\n",
    "posts_sampled_balanced.to_csv('data/cleaned/posts_sampled_balanced.csv', index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_sampled_balanced= pd.read_csv('data/cleaned/posts_sampled_balanced.csv', quoting=csv.QUOTE_ALL, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do a train/test split\n",
    "X_train_posts, X_test_posts, y_train_posts, y_test_posts = train_test_split(posts_sampled_balanced['full_text'], posts_sampled_balanced['is_science'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29690, 14624, 29690, 14624)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_posts), len(X_test_posts), len(y_train_posts), len(y_test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_clf = text_clf_pipeline.fit(X_train_posts, y_train_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_post = post_clf.predict(X_test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8354759299781181"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_posts, pred_post, average='micro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8364332603938731"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_post == y_test_posts)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_post_est = joblib.load('best_post_est.pkl')\n",
    "post_grid_results = joblib.load('post_grid_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9c3d634ca0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_comments_2018_unparsed.jsonlist'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reddit/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('all_comments_2018_unparsed.jsonlist') as f:\n",
    "    comments = f.readlines() \n",
    "comments = [json.loads(line) for line in comments] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/taugust/Projects/ARK/community_guidelines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/homes/gws/taugust/Projects/ARK/community_guidelines')\n",
    "\n",
    "%run Reddit/SRILM_building_funcs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing  \u001b[35mdata/cleaned/sub_comments/funny_comments_2018.csv\u001b[0m ..... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/taugust/miniconda3/envs/reddit/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/funny_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/science_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/science_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/news_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/news_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/politics_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/politics_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/pics_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/pics_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/AskReddit_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/AskReddit_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/AskHistorians_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/AskHistorians_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/EverythingScience_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/EverythingScience_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/Futurology_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/Futurology_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/TrueReddit_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/TrueReddit_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/dataisbeautiful_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/dataisbeautiful_author_counts_train_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/sub_comments/askscience_comments_2018.csv\u001b[0m ..... Done\n",
      "Importing  \u001b[35mdata/cleaned/train/2018/author_counts/askscience_author_counts_train_2018.csv\u001b[0m ..... Done\n"
     ]
    }
   ],
   "source": [
    "# Getting entire comment set fro\n",
    "subs = ['funny', 'science', 'news', 'politics', 'pics', 'AskReddit', 'AskHistorians', 'EverythingScience', 'Futurology', 'TrueReddit', 'dataisbeautiful', 'askscience']\n",
    "\n",
    "df_comment_list = []\n",
    "\n",
    "for s in subs:     \n",
    "    df_comments, df_author_counts_train = import_csvs(s, path='data/cleaned/train/2018/', ext='_train_2018.csv', comment_pre_path='data/cleaned/sub_comments/', comment_ext='_comments_2018.csv')\n",
    "    df_comment_list.append(df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/taugust/miniconda3/envs/reddit/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_comment_total = pd.concat(df_comment_list, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = df_comment_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    66062089\n",
      "True       604267\n",
      "Name: is_science, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "comments_df['is_science'] = comments_df['subreddit'] == 'science'\n",
    "print(comments_df['is_science'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_not_science = comments_df[~comments_df['is_science']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_science = comments_df[comments_df['is_science']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_sampled_not_science = comments_df_not_science.sample(len(comments_df_science), random_state=42)\n",
    "comments_sampled_balanced = comments_df_sampled_not_science.append(comments_df_science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sampled_balanced.to_csv('data/cleaned/comments_sampled_balanced.csv', index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sampled_balanced = pd.read_csv('data/cleaned/comments_sampled_balanced.csv', quoting=csv.QUOTE_ALL, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208534"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_sampled_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df_sampled = posts_df[['full_text', 'is_science']].sample(100000, random_state=42)\n",
    "# print(len(comments_df_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comments, X_test_comments, y_train_comments, y_test_comments = train_test_split(comments_sampled_balanced['body'], comments_sampled_balanced['is_science'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(809717, 398817, 809717, 398817)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_comments), len(X_test_comments), len(y_train_comments), len(y_test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/taugust/miniconda3/envs/reddit/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# grid_comment = GridSearchCV(text_clf_pipeline, parameters, cv=2, verbose=1, scoring='f1')\n",
    "# grid_comment.fit(X_train_comments, y_train_comments)\n",
    "comment_clf = text_clf_pipeline.fit(X_train_comments, y_train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment_clf_sampled.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(comment_clf, 'comment_clf_sampled.pkl', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_comment = comment_clf.predict(X_test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7252173302542269"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test_comments == pred_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7252173302542269"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_comments, pred_comment, average='micro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(grid_comment.best_estimator_, 'best_comment_est.pkl', compress = 1)\n",
    "# joblib.dump(grid_comment.cv_results_, 'comment_grid_results.pkl', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_comment.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf.fit(X_train_comments, y_train_comments) \n",
    "# predicted = text_clf.predict(X_test_comments)\n",
    "# print('comment accuracy:', np.mean(predicted == y_test_comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# science_comments = [line for line in comments if line['subreddit'] == 'science']\n",
    "# print(len(science_comments))\n",
    "# background_comments = [line for line in comments if line['subreddit'] != 'science']\n",
    "# print(len(background_comments))\n",
    "\n",
    "# science_text_comments = [line['body'] for line in science_comments]\n",
    "# background_text_comments = [line['body'] for line in background_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the texts but with seperate labels\n",
    "# science_text_labeled_comments = [(0, line) for line in science_text_comments]\n",
    "# background_text_labeled_comments = [(1, line) for line in background_text_comments]\n",
    "# all_text_labled_comments = science_text_labeled_comments + background_text_labeled_comments\n",
    "\n",
    "# # do a train/test split\n",
    "# X_train_comments, X_test_comments, y_train_comments, y_test_comments = train_test_split([line[1] for line in all_text_labled_comments], [line[0] for line in all_text_labled_comments] , test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882961310833319"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "reddit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
